---
title: "Calibraci贸n de probabilidades"
subtitle: "Estimaci贸n de default mediante modelos de machine learning"
author: Karina Bartolom茅
institute: "Organizadora: Natalia Salaberry <br> CIMBAGE (IADCOM) - Facultad Ciencias Econ贸micas (UBA)"
date: 2024-05-13
format: 
    revealjs:
        theme: [default, custom_testing.scss]
        logo: logo-uba.jpeg
        footer: |
            <body>
            Facultad de Ciencias Econ贸micas - UBA   |  
            {{< fa brands github size=1x >}} [github.com/karbartolome/workshops](https://github.com/karbartolome/workshops)
            </body>
        self-contained: true
        embed-resources: true
        slide-number: true
        toc: true
        toc-depth: 1
        number-sections: true
        number-depth: 2
        title-slide-attributes:
            data-background-size: contain  
# format: beamer
jupyter: 
  kernelspec:
    name: "quarto-env"
    language: "python"
    display_name: "quarto-env"
execute:
  echo: false
  warning: false
  code-fold: false
  layout-align: center
lang: es
---

## Flow

```{mermaid}
flowchart TD
    a --> b
```


## A

```{python}
#| echo: false
import seaborn as sns
import pandas as pd
import numpy as np
import sklearn
import matplotlib
import great_tables
from IPython.display import display, Markdown, Latex
Markdown(f"""
**Procesamiento de datos**:\n
 pandas=={pd.__version__}\n
 numpy=={np.__version__}\n
\n
**Modelado**:\n
 scikit-learn=={sklearn.__version__}\n
\n
**Visualizaci贸n y tablas:**\n
 matplotlib=={matplotlib.__version__}\n
 seaborn=={sns.__version__}\n
 great_tables=={great_tables.__version__}\n
""")
```

```{python}
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
```

## Evaluaci贸n de un modelo de clasificaci贸n (Cont.)
::: {style="font-size: 50%;"}

Siendo $N$ la cantidad de observaciones, $y_i$ el valor observado para la obseravci贸n $i$ y $\hat{y_i}$ el valor predicho para la observaci贸n $i$, se definen las funciones de p茅rdida:

::: {.nav-pills .panel-tabset}
## Log loss
Tambi茅n denominada Negative Mean Log-Likelihood

::: columns
::: {.column width="50%"}
$$
\text{Log Loss} = -\frac{1}{N} \sum_{i=1}^{N} \left( \color{red}{y_i \log(\hat{y_i})} + \color{blue}{(1 - y_i) \log(1 - \hat{y_i})} \right)
$$

> Si $y_i=1$ => $\text{Log Loss}=\color{red}{\log(\hat{y_i})}$
>
> Si $y_i=0$ => $\text{Log Loss}=\color{blue}{\log(1 - \hat{y_i})}$
:::
::: {.column width="50%"}
```{python}
#| echo: true
#| code-fold: true
#| code-summary: "C贸digo"
#| layout-align: center
#| fig-cap: "Logatitmo de una probabilidad"
#| label: fig-log
N=100
temp = (pd.DataFrame({
    'y_pred':[i/N for i in range(1,N,1)]
    })
    .assign(y_pred_log = lambda x: np.log(x['y_pred']))
).plot(x='y_pred', y='y_pred_log', figsize=(3,3), xlabel='Prob', ylabel='Log(Prob)');
```
:::
:::

- La p茅rdida es 0 si se tiene seguridad sobre una predicci贸n y la predicci贸n es correcta
- La p茅rdida es $\infty$ si se tiene seguridad sobre una predicci贸n y la predicci贸n es incorrecta

## Brier loss
Error Cuadr谩tico Medio pero para clasificaci贸n:

$$
\text{Brier Loss} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y_i})^2
$$

:::
:::



# Great tables

## Tabla

Un texto para probar el tama帽o del texto!

```{python}
great_tables.GT(pd.DataFrame({'a':[1,2,3],'b':['a','b','c']}))
```


# Temp

## Temp b

::: {.nav-pills}
::: {.panel-tabset}

```{python}
#| output: asis
df = pd.DataFrame({
  'var':['a']*10+['b']*3,
  'value':list(range(0,10))+list(range(0,3))
})

for i in ['a','b']:
    display(Markdown(f"## {i}"))
    display(Markdown("::: {.cell tbl-cap='"+i+"'}\n"))
    display(Markdown("::: {.cell label='tbl-"+i+"'}\n"))
    display(great_tables.GT(df.query('var==@i')))
    display(Markdown(":::\n"))
    display(Markdown(":::\n"))
```
:::
:::

# Nueva secci贸n

## Temp

@tbl-a es una tabla y @tbl-b es otra

## Contacto

{{< fa link >}} [Blog](https://karbartolome-blog.netlify.com)

{{< fa brands twitter size=1x >}} [@karbartolome](https://twitter.com/karbartolome)

{{< fa brands github size=1x >}} [@karbartolome](http://github.com/karbartolome)