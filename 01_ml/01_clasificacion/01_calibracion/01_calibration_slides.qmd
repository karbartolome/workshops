---
title: "Calibración de probabilidades"
subtitle: "Estimación de default mediante modelos de machine learning"
author: Karina Bartolomé
institute: "Organizadora: Natalia Salaberry <br> CIMBAGE (IADCOM) - Facultad Ciencias Económicas (UBA)"
date: 2024-05-13
format: 
    revealjs:
        theme: [default, custom.scss]
        logo: logo-uba.jpeg
        footer: |
            <body>
            Facultad de Ciencias Económicas - UBA  | 
            <a href="https://github.com/karbartolome/workshops">
                <img src="https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg" alt="GitHub" width="20" height="20">
            </a>
            </body>
        self-contained: true
        embed-resources: true
        slide-number: true
        toc: true
        toc-depth: 1
        number-sections: true
        number-depth: 2
        title-slide-attributes:
            data-background-size: contain  
#format: beamer
jupyter: 
  kernelspec:
    name: "quarto-env"
    language: "python"
    display_name: "quarto-env"
execute:
  echo: false
  warning: false
  code-fold: false
  layout-align: center
lang: es
---

```{python}
#| echo: false
#| output: false
%load_ext autoreload
%autoreload 2
```

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Librerías"
import pandas as pd
import numpy as np
from copy import deepcopy
import seaborn as sns
import matplotlib.pyplot as plt
from collections import Counter
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.compose import make_column_selector
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier
from sklearn.calibration import calibration_curve, CalibratedClassifierCV
from sklearn.metrics import (
    classification_report,
    roc_auc_score,
    roc_curve,
    RocCurveDisplay,
    log_loss,
    recall_score,
    brier_score_loss,
    confusion_matrix,
    ConfusionMatrixDisplay,
)
from IPython.display import display
from custom_functions import (
    plot_distribution, plot_calibration_models, gain_table
)
from great_tables import GT, from_column, style, loc
from matplotlib.colors import ListedColormap, to_rgb

import sklearn
sklearn.set_config(transform_output="pandas")
```

```{python}
#| echo: false
#| code-fold: false
color_verde = "#255255"
color_verde_claro = "#BDCBCC"
```

# Introducción a machine learning

::: {style="font-size: 50%;"}
## ¿Qué es un modelo de clasificación?

Se busca predecir la [probabilidad de ocurrencia]{style="color: blue"} de un evento a partir de ciertas características observables:

> $P(y=1) = f(X)$
>
> *Siendo y una variable que puede tomar 2 valores: 0 o 1*

::: {.fragment .fade-in}
<br> 
**Ejemplos de clasificación:**

▪️**Iris**: Clasificación de especies de plantas

▪️**Titanic**: Probabilidad de supervivencia

::: {.fragment .highlight-red}
▪️**German Credit**: Probabilidad de default
:::
:::
:::


# Caso: German Credit

## Datos

::: {style="font-size: 50%;"}

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Código"
PATH_DATA = 'https://raw.githubusercontent.com/ayseceyda/german-credit-gini-analysis/master/gc.csv'
TARGET = "Risk"

df = (pd.read_csv(PATH_DATA)
  .drop('Unnamed: 0', axis=1)
  .assign(Risk = lambda x: np.where(x['Risk']=='good',0,1))
)
```

Se cuenta con un dataset de `{python} df.shape[0]` observaciones y `{python} df.shape[1]` variables.

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Código"
#| tbl-cap: "Datos de German Credit (muestra de 6 observaciones)"
#| label: tbl-credit-data
def map_color(data):
    return (data[TARGET] == 1).map(
        {True: color_verde_claro, False: 'white'}
    )

(GT(df.sample(6, random_state=123)
        .set_index(TARGET)
        .reset_index()
    )
    .fmt_currency(columns="Credit amount")
    .tab_style(
        style=style.fill(color=map_color), locations=loc.body(columns=df.columns.tolist()),
    )
    .tab_style(
        style=style.text(color='red', weight = "bold"), locations=loc.body(TARGET),
    )
    .tab_options(
        column_labels_background_color=color_verde,
        table_font_names="Times New Roman"
    )
)
```

La variable objetivo (target) es Risk, donde el porcentaje de observaciones de clase 1 (riesgosos) es `{python} f"{df['Risk'].sum()/df.shape[0]:.1%}"`

:::


## Particiones
::: {style="font-size: 50%;"}

Partición en dataset de entrenamiento, validación y evaluación.

- Dataset de entrenamiento --> Ajuste del modelo
- Dataset de validación --> Calibración del modelo
- Dataset de evaluación --> Métricas del modelo calibrado

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Código"
#| label: particiones
y = df[TARGET]
X = df.drop([TARGET], axis=1)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.5, shuffle=True, stratify=y, random_state=42
)

# Lo ideal es utilizar la partición de validación para calibración
X_valid, X_test, y_valid, y_test = train_test_split(
    X_test, y_test, test_size=0.5, shuffle=True, stratify=y_test, random_state=42
)

print(f"N observaciones en entrenamiento: {X_train.shape[0]}")
print(f"N observaciones en validación: {X_valid.shape[0]}")
print(f"N observaciones en evaluación: {X_test.shape[0]}")
```

::: 


::: {style="font-size: 50%;"}
## Modelo de clasificación simple

Se consideran 2 variables para ajustar un modelo de arbol de decisión con máxima profundidad=2

$$
P(\text{Risk}=1) = f(\text{Credit amount}, \text{Age})
$$

<br>

::: columns
::: {.column width="50%"}

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Código"
#| tbl-cap: "Datos de German Credit (2 variables)"
#| label: tbl-credit-data-1var
subset_cols = ['Age', 'Credit amount']

(GT(pd.concat([y_train, X_train], axis=1)
        .sample(6, random_state=123)
        .set_index(TARGET)
        [subset_cols]
        .reset_index()
    )
    .fmt_currency(columns="Credit amount")
    .tab_style(
        style=style.fill(color=map_color), locations=loc.body(columns=df.columns.tolist()),
    )
    .tab_style(
        style=style.text(color='red', weight = "bold"), locations=loc.body(TARGET),
    )
    .tab_options(
        column_labels_background_color=color_verde,
        table_font_names="Times New Roman"
    )
)
```

Ajuste del modelo:

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Código"
X_train_subset = X_train[subset_cols].copy()
clf = DecisionTreeClassifier(max_depth=2).fit(X_train_subset, y_train)
clf
```

:::



::: {.column width="50%"}

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Código"
#| layout-align: center
#| fig-cap: "Modelo de árbol de decisión"
#| label: fig-credit-tree
colors = [color_verde, 'red']
labels = ['No riesgoso','Riesgoso']
plt.figure(figsize=(6.5,6.5))
arbol = plot_tree(
    clf,
    filled=True,
    impurity=False,
    feature_names=X_train_subset.columns.tolist(),
    class_names=labels,
    fontsize=8
)
for i, impurity, value in zip(arbol, clf.tree_.impurity, clf.tree_.value):
    # let the max value decide the color; whiten the color depending on impurity (gini)
    r, g, b = to_rgb(colors[np.argmax(value)])
    f = impurity * 2 # for N colors: f = impurity * N/(N-1) if N>1 else 0
    i.get_bbox_patch().set_facecolor((f + (1-f)*r, f + (1-f)*g, f + (1-f)*b))
    i.get_bbox_patch().set_edgecolor('black')
plt.show()
```
:::
:::
:::

::: {style="font-size: 50%;"}
## Evaluación de un modelo de clasificación
::: columns
::: {.column width="50%"}

Se utiliza el método `clf.predict()`para predecir la clase (0,1) según el modelo (`clf`). A partir de la clase predicha se obtiene la matriz de confusión:

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Ver código"
#| label: tbl-cm
#| tbl-cap: 'Matriz de confusión'
preds = pd.DataFrame({'Valor observado':y_test,'Valor predicho':clf.predict(X_test[subset_cols])})
display(GT(preds
    .groupby(['Valor observado','Valor predicho'], as_index=False).size()
    .pivot(index='Valor observado', columns='Valor predicho', values='size')
    .rename({0:'0',1:'1'}, axis=1)
    .reset_index()
    )
    .tab_spanner('Valor predicho', columns = ['0','1'])
    .data_color(
        columns=['0','1'],
        domain=[0,X.shape[0]],
        palette=[color_verde_claro, 'red'],
        na_color="white",
    )
)
```
:::

::: {.column width="50%"}

Además de predecir una clase, sklearn permite predecir una "probabilidad" con `clf.predict_proba()`:

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Ver código"
#| label: tbl-predictproba
#| tbl-cap: 'clf.predict_proba(), muestra de 2 observaciones'
y_pred_proba = clf.predict_proba(X_test[subset_cols])[:, 1]
preds = pd.DataFrame({
    'y_obs':y_test,
    'predict_proba':y_pred_proba
})
GT(preds.sample(2, random_state=42)).fmt_number('predict_proba')
```

```{python}
metrica_auc = roc_auc_score(y_true=y_test, y_score=y_pred_proba)
metrica_logloss = log_loss(y_true=y_test, y_pred=y_pred_proba)
metrica_brierloss = brier_score_loss(y_true=y_test, y_prob=y_pred_proba)

print(f"""
ROC AUC = {metrica_auc:.2}, Log loss = {metrica_logloss:.2}, Brier loss = {metrica_brierloss:.2}
""")
```

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Ver código"
#| label: fig-distrib-tree
#| fig-cap: 'Distribución de "probabilidad" predicha'
plot_distribution(data=preds, pred_column='predict_proba', class_0_color=color_verde)
```


:::
:::
:::

::: {style="font-size: 50%;"}
## Preprocesamiento

Se construye un pipeline de preprocesamiento de variables, diferenciando el procesamiento de variables categóricas y numéricas

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Ver código"
numeric_transformer = Pipeline([
    ('impute', SimpleImputer(strategy='median')),
    ('scaler', MinMaxScaler())
])

categorical_transformer = Pipeline([
    ('ohe',OneHotEncoder(
        min_frequency=0.05,
        handle_unknown='infrequent_if_exist',
        sparse_output=False)
    )
])

preproc = ColumnTransformer([
    ('num', numeric_transformer,
      make_column_selector(dtype_include=['float','int'])),
    ('cat', categorical_transformer,
      make_column_selector(dtype_include=['object','category']))
], verbose_feature_names_out=False)
```

```{python}
#| echo: false
#| code-fold: false
#| classes: custom_class_html_display
preproc
```

::: 

::: {style="font-size: 50%;"}
## Modelado
```{python}
model = RandomForestClassifier(random_state=42)
model = LogisticRegression(random_state=42)
model = HistGradientBoostingClassifier(
    random_state=42,
    max_depth=4,
    learning_rate=0.1,
    class_weight={1:1,0:1}, # Sin pesos para las clases
    max_iter=1000
)

# from sklearn.svm import SVC
# model = SVC(random_state=42, probability=True)

pipe = Pipeline([
    ('preproc', preproc),
    ('model', model)
])
pipe.fit(X_train, y_train)
```
:::

## Evaluación

```{python}
print(classification_report(y_test, pipe.predict(X_test)))
y_test_pred_proba = pipe.predict_proba(X_test)[:,1]
metrica_auc = roc_auc_score(y_true=y_test, y_score=y_test_pred_proba)
metrica_logloss = log_loss(y_true=y_test, y_pred=y_test_pred_proba)
metrica_brierloss = brier_score_loss(y_true=y_test, y_prob=y_test_pred_proba)

print(f"""
ROC AUC = {metrica_auc:.2}
Log loss = {metrica_logloss:.2}
Brier loss = {metrica_brierloss:.2}
""")
```


## Problema

Planteo de problema organizacional


# Calibración de probabilidades

::: {style="font-size: 50%;"}
## Calibración de probabilidades

**Objetivo:** 

Se busca encontrar una función que ajuste la relación entre los scores predichos por el modelo (predict_proba) y las probabilidades reales

> $P(y_i=1)= f(z)$
>
> Siendo: $P(y_{i}=1)$ la probabilidad calibrada para el individuo $i$ y $z_{i}$ el output del modelo no calibrado (score)

<br>

Un clasificador binario bien calibrado debería clasificar de forma tal que para las observaciones que predice un score (predict_proba) = 0.8, se espera que un 80% de las observaciones correspondan a la clase positiva (1).

<br>

**Referencias:**

- [Calibración de probabilidades en {scikit-learn} 📦 ](https://scikit-learn.org/stable/modules/calibration.html)

<br>

::: {.fragment .fade-in}
**Métodos de calibración**

::: {.fragment .highlight-red}
 Calibración Sigmoide (Platt scaling)

 Calibración Isotónica
:::

 Calibración Beta (Nuevo)

 Calibración Spline (Nuevo)
:::
:::

::: {style="font-size: 50%;"}
## Calibración sigmoide (Platt scaling)

Este método asume una relación logística entre los scores (z) y la probabilidad real (p):

> $log(\frac{p}{1-p})=\alpha+\beta(z_{i})$ 
>
>
> $P(y_{i}=1) = \frac{1}{1+(e^{-(α+β(z_{i}))})}$
>
>
> Siendo: $y_{i}$ el valor observado para el individuo $i$ y $z_{i}$ el output del modelo no calibrado

<br>
Se estiman 2 parámetros ($\alpha$ y $\beta$), como en una regresión logística. 

- Requiere pocos datos

- Es útil cuando el modelo no calibrado tiene errores similares en predicción de valores bajos y altos 

<br>
**Referencias:**

- Platt, John. (2000). Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods. Adv. Large Margin Classif, Volume 10 (pp. 61-74).

:::

::: {style="font-size: 50%;"}
## Calibración sigmoide (Platt scaling) (Cont.)

```{python}
preds = pd.DataFrame({
    'y_obs': y_test,
    'pred_class': pipe.predict(X_test),
    'pred_prob': pipe.predict_proba(X_test)[:,1],
})
pipe_sigmoid = CalibratedClassifierCV(pipe, cv='prefit', method="sigmoid")
pipe_sigmoid.fit(X_valid, y_valid)

prob_pos_sigmoid = pipe_sigmoid.predict_proba(X_test)[:, 1]
preds['pred_sigmoid'] = prob_pos_sigmoid
preds['pred_class_sigmoid']=[1 if i>0.5 else 0 for i in prob_pos_sigmoid]
```

```{python}
# Cálculo manual
# clf_log = LogisticRegression(random_state=42, C=0.1, solver="lbfgs")
# y_valid_pred_uncalibrated = pipe.predict_proba(X_valid)[:, 1]
# clf_log.fit(X=pd.DataFrame({"pred_prob": y_valid_pred_uncalibrated}), y=y_valid)

# preds["pred_sigmoid_manual"] = clf_log.predict_proba(preds[["pred_prob"]])[:, 1]
```

```{python}
# | echo: true
# | code-fold: true
# | code-summary: "Ver código"
# | layout-ncol: 2
# | label: fig-calibration-sigmoid
# | fig-cap: ['Calibración (bins)','Calibración (ajuste)']
cal_data = plot_calibration_models(
    y_obs=preds["y_obs"],
    y_pred_prob=preds["pred_prob"],
    y_pred_prob_cal=preds["pred_sigmoid"],
    bins=7,
    strategy="uniform",
    figsize=(4,4),
    display_bins=True
)
plt.legend(loc='center left', bbox_to_anchor=(0, -0.3))
plt.show()

# display(GT(cal_data).fmt_number(['prob_true','prob_pred']))

plt.figure(figsize=(4,4))
sns.scatterplot(
    x="pred_prob", y="pred_sigmoid", data=preds,
    color="blue", alpha=0.7, label="Calibración sklearn",
)
sns.scatterplot(
    x="pred_prob", y="y_obs", data=preds,
    alpha=0.3, color="black", label="Valores observados",
)
plt.axline([0, 0], [1, 1], label="Calibración perfecta", c="red")
plt.ylabel("Probabilidad calibrada")
plt.xlabel("Probabilidad no calibrada")
plt.legend(loc='center left', bbox_to_anchor=(0, -0.3))
plt.ylim([-0.01, 1.01]);
```

```{python}
# Verificar por qué no funciona
# plt.figure()
# sns.scatterplot(
#     x="pred_prob",
#     y="pred_sigmoid_manual",
#     data=preds,
#     alpha=0.7,
#     label="Calibración manual",
# )
```



:::

::: {style="font-size: 50%;"}
## Calibración isotónica

```{python}
pipe_isotonic = CalibratedClassifierCV(pipe, cv='prefit', method="isotonic")
pipe_isotonic.fit(X_valid, y_valid)
```


```{python}
prob_pos_sigmoid = pipe_sigmoid.predict_proba(X_test)[:, 1]
prob_pos_isotonic = pipe_isotonic.predict_proba(X_test)[:, 1]

preds = pd.DataFrame({
    'y_obs': y_test,
    'pred_class': pipe.predict(X_test),
    'pred_prob': pipe.predict_proba(X_test)[:,1],
    'pred_sigmoid': prob_pos_sigmoid,
    'pred_class_sigmoid': [1 if i>0.5 else 0 for i in prob_pos_sigmoid],
    'pred_isotonic': prob_pos_isotonic,
    'pred_class_isotonic': [1 if i>0.5 else 0 for i in prob_pos_isotonic],
})

print(f'% observado: {y_test.sum()/len(y_test):.3}')
print(f'Prob promedio: {preds.pred_prob.mean():.3}')
print(f'Prob promedio (calibración sigmoide): {prob_pos_sigmoid.mean():.3}')
print(f'Prob promedio (calibración isotónica): {prob_pos_isotonic.mean():.3}')
```

:::

## Comparación de modelos
::: {style="font-size: 50%;"}


```{python}
# Métricas de ordenamiento
print(f"ROC AUC modelo={roc_auc_score(y_test, preds.pred_prob):.3f}")
print(f"ROC AUC modelo calibrado={roc_auc_score(y_test, preds.pred_sigmoid):.3f}")

# Métricas de probabilidad
print(f"Log loss modelo={log_loss(y_test, preds.pred_prob):.3f}")
print(f"Log loss modelo calibrado={log_loss(y_test, preds.pred_sigmoid):.3f}")
print(f"Brier modelo={brier_score_loss(y_test, preds.pred_prob):.3f}")
print(f"Brier modelo calibrado={brier_score_loss(y_test, preds.pred_sigmoid):.3f}")

# Métricas de clasificación (tomando como punto de corte prob=0.5)
print(f"Recall modelo={recall_score(y_test, preds.pred_class):.3f}")
print(f"Recall modelo calibrado={recall_score(y_test, preds.pred_class_sigmoid):.3f}")
```

Se visualizan las distribuciones de probabilidad predicha según el modelo calibrado y no calibrado.

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Ver código"
#| layout-ncol: 2
#| fig-cap: ['Modelo no calibrado','Modelo calibrado (sigmoide)']
plot_distribution(
    data=preds, pred_column='pred_prob'
)
plot_distribution(
    data=preds, pred_column='pred_sigmoid'
)
```

:::

## Visualización
::: {style="font-size: 50%;"}


```{python}
#| layout-ncol: 2
BINS = int(np.sqrt(preds.shape[0])/2)
plot_calibration_models(
        y_obs=preds['y_obs'],
        y_pred_prob=preds['pred_prob'],
        y_pred_prob_cal=preds['pred_sigmoid'],
        bins=BINS,
        strategy='uniform'
    )

plot_calibration_models(
        y_obs=preds['y_obs'],
        y_pred_prob=preds['pred_prob'],
        y_pred_prob_cal=preds['pred_isotonic'],
        bins=BINS,
        strategy='uniform'
    )
```

:::

## Tablas de ganancias
::: {style="font-size: 50%;"}

Se busca un modelo en donde la probabilidad promedio de cada bin se corresponda con el % de clase positiva observado en ese bin según las predicciones del modelo.

Esto es útil para la toma de decisiones, ya que permite establecer punto de cortes diferenciales en función de la aversión en riesgo de la entidad.

Por ejemplo:

- En un modelo de scoring crediticio, otorgarle créditos a N individuos con probabilidad en cierto intervalo tiene un riesgo asociado (mora esperada)
- En un modelo de churn (abandono) de clientes, otorgarle una promoción a individuos de cierto intervalo de probabilidad de abandono tiene un costo asociado (descuento para individuos que no abandonarían)
- Entre otros.

:::

## Tabla de ganancias (Cont.)
::: {style="font-size: 50%;"}


```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Código"
#| tbl-cap: "Tabla de ganancias (modelo no calibrado)"
#| label: tbl-gain
gain_table(preds=preds, bins=5)
```

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Código"
#| tbl-cap: "Tabla de ganancias (calibración sigmoide)"
#| label: tbl-gain-sigmoid
gain_table(preds=preds, pred_column='pred_sigmoid', bins=5)
```

:::
