{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b579e584",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle \n",
        "from matplotlib.colors import ListedColormap, to_rgb\n",
        "from great_tables import GT, from_column, style, loc\n",
        "from collections import Counter\n",
        "\n",
        "# from sklearn import datasets\n",
        "# Modelado\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.compose import ColumnTransformer, make_column_selector\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from catboost import CatBoostClassifier\n",
        "from IPython.display import display, Markdown, Latex\n",
        "\n",
        "import sklearn\n",
        "sklearn.set_config(transform_output=\"pandas\", display='diagram')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dccad6d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "color_verde = \"#255255\"\n",
        "color_verde_claro = \"#BDCBCC\"\n",
        "target = 'is_fraud'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ca5e162",
      "metadata": {},
      "source": [
        "## Consideraciones previas\n",
        "\n",
        "::: {style=\"font-size: 70%;\"}\n",
        "\n",
        "- Este taller está enfocado en cuestiones avanzadas sobre el flujo de modelado de datos en python, se recomienda algún conocimiento previo para un mejor entendimiento del código. \n",
        "\n",
        "- Durante el seminario se utilizarán los siguientes paquetes (**python**). \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "377287fb",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: false\n",
        "import matplotlib\n",
        "import great_tables\n",
        "import catboost\n",
        "from IPython.display import display, Markdown, Latex\n",
        "Markdown(f\"\"\"\n",
        "📦 pandas=={pd.__version__}\\n\n",
        "📦 numpy=={np.__version__}\\n\n",
        "📦 scikit-learn=={sklearn.__version__}\\n\n",
        "📦 catboost=={catboost.__version__}\\n\n",
        "📦 matplotlib=={matplotlib.__version__}\\n\n",
        "📦 seaborn=={sns.__version__}\\n\n",
        "📦 great_tables=={great_tables.__version__}\\n\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2459e67a",
      "metadata": {},
      "source": [
        "- Todo el código se encuentra disponible en {{< fa brands github size=1x >}} <a href=\"https://github.com/karbartolome/workshops\" target=\"_blank\">Repositorio</a>\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "# Planteo del caso\n",
        "\n",
        "## Prevención del fraude transaccional\n",
        "\n",
        "::: {style=\"font-size: 70%;\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5e8c3c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "#| code-fold: true\n",
        "#| code-summary: \"Lectura de datos\"\n",
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"kartik2112/fraud-detection\")\n",
        "df_train = pd.read_csv(f\"{path}/fraudTrain.csv\")\n",
        "df_test = pd.read_csv(f\"{path}/fraudTest.csv\")\n",
        "df = pd.concat([df_train, df_test], axis=0, ignore_index=True).reset_index(drop=True)\n",
        "\n",
        "target = 'is_fraud'\n",
        "cols_selected = [\n",
        "    \"trans_date_trans_time\",\n",
        "    \"merchant\",\n",
        "    \"category\",\n",
        "    \"amt\",\n",
        "    \"city_pop\",\n",
        "    \"job\",\n",
        "    \"dob\",\n",
        "    \"lat\",\n",
        "    \"long\",\n",
        "    \"merch_lat\",\n",
        "    \"merch_long\",\n",
        "    \"is_fraud\",\n",
        "]\n",
        "df = df[cols_selected]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98f4b473",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: false\n",
        "# import kagglehub\n",
        "# path = kagglehub.dataset_download(\"kartik2112/fraud-detection\")\n",
        "# df_train = pd.read_csv(f\"{path}/fraudTrain.csv\")\n",
        "# df_test = pd.read_csv(f\"{path}/fraudTest.csv\")\n",
        "# df = pd.concat([df_train, df_test], axis=0, ignore_index=True).reset_index(drop=True)\n",
        "# df.to_parquet('df_fraud.parquet', index=False, engine='fastparquet')\n",
        "df = pd.read_parquet('data/df_fraud.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "268dffaf",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: false\n",
        "# df = df.sample(10000, random_state=42).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b421e9a1",
      "metadata": {},
      "source": [
        "Se cuenta con un dataset de `{python} df.shape[0]` transacciones de tarjetas de crédito. Son `{python} df.shape[1]` variables generadas sintéticamente por lo que el foco está sobre cómo procesarlos y no sobre la performance del modelo [^1].\n",
        "\n",
        "No se cuenta con valores faltantes, por lo que se genera \"ruido\" en el dataset, añadiendo valores faltantes en distintas variables para el ejemplo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a0482e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| code-fold: true\n",
        "#| code-summary: \"Generador de ruido en el dataset\"\n",
        "## Valores faltantes\n",
        "def add_random_nans(values, fraction=0.2):\n",
        "    \"\"\"\n",
        "    Generador de valores faltantes\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "    mask = np.random.rand(len(values)) < fraction\n",
        "    new_values = values.copy()\n",
        "    new_values[mask] = np.nan\n",
        "    return new_values\n",
        "\n",
        "df = df.assign(\n",
        "    merchant = lambda x: [i.replace('fraud_','') for i in x['merchant']],\n",
        "    dob = lambda x: add_random_nans(x['dob'], fraction=0.05),\n",
        "    job = lambda x: add_random_nans(x['job'], fraction=0.1),\n",
        "    city_pop = lambda x: add_random_nans(x['city_pop'], fraction=0.03),\n",
        "    merch_lat = lambda x: add_random_nans(x['merch_lat'], fraction=0.02),\n",
        "    merch_long = lambda x: add_random_nans(x['merch_long'], fraction=0.02),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a175ba02",
      "metadata": {},
      "source": [
        "> Ante una nueva transacción, ¿cuál es la probabilidad de que sea fraudulenta? ¿Debería bloquarse?\n",
        "\n",
        "![Diagrama caso](images/diagrama_caso.png){#fig-caso width=\"30%\" fig-align=\"center\"}\n",
        ":::\n",
        "\n",
        "[^1]: @calibracion2024 es un workshop similar a este, enfocadao en la performance de modelos de clasificación binaria.\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "::: {style=\"font-size: 75%;\"}\n",
        "La variable objetivo (target) es [is_fraud]{style=\"color:#FF9933\"} , donde el porcentaje de observaciones de clase 1 (fraudulentos) es `{python} f\"{df['is_fraud'].sum()/df.shape[0]:.2%}\"`.\n",
        "\n",
        "> $P(\\color{#FF9933}{is\\_fraud}=1) = f(\\color{#3399FF}{X})$\n",
        ">\n",
        "> $\\color{#FF9933}{is\\_fraud}$: *variable que puede tomar 2 valores: 1 (transacción fraudulenta) o 0 (transacción legítima)*\n",
        ">\n",
        "> $\\color{#3399FF}{X}$: *matriz nxm, siendo n la cantidad de observaciones y m la cantidad de variables (o atributos)*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tbl-fraud-data",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: false\n",
        "#| code-fold: false\n",
        "#| tbl-cap: Datos transaccionales (muestra de 4 observaciones)\n",
        "#| label: tbl-fraud-data\n",
        "def map_color(data):\n",
        "    return (data[target] == 1).map(\n",
        "        {True: color_verde_claro, False: 'white'}\n",
        "    )\n",
        "\n",
        "(GT(df.sample(4, random_state=13).round(2)\n",
        "        .set_index(target)\n",
        "        .reset_index()\n",
        "    )\n",
        "    .fmt_currency(columns=\"amt\")\n",
        "    .tab_style(\n",
        "        style=style.fill(color=map_color), locations=loc.body(columns=df.columns.tolist()),\n",
        "    )\n",
        "    .tab_style(\n",
        "        style=style.text(color='#FF9933', weight = \"bold\"), locations=loc.body(target),\n",
        "    )\n",
        "    .tab_options(\n",
        "        column_labels_background_color=color_verde,\n",
        "        table_font_names=\"Times New Roman\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99c677d1",
      "metadata": {},
      "source": [
        "Fuente de los datos: [Credit Card Transactions Fraud Detection Dataset](https://www.kaggle.com/datasets/kartik2112/fraud-detection/data).\n",
        ":::\n",
        "\n",
        "# Esquema de modelado\n",
        "\n",
        "## Esquema de modelado\n",
        "\n",
        "::: {style=\"font-size: 75%;\"}\n",
        "La Figura @fig-esquema muestra un posible esquema de trabajo para modelos de aprendizaje automático en donde se busca predecir sobre datos nuevos. @quemy2019two destaca la importancia de diseñar primero el flujo de datos y luego destinar tiempo al ajuste de hiperparámetros del modelo.\n",
        "\n",
        "![Diagrama caso](images/diagrama_esquema.svg){#fig-esquema width=\"70%\" fig-align=\"center\"}\n",
        ":::\n",
        "\n",
        "## Particiones\n",
        "\n",
        "::: {style=\"font-size: 75%;\"}\n",
        "\n",
        "El dataset se separa en un segmento para [entrenamiento (train)]{style=\"color:#4D9900; font-weight:bold\"} y otro para la [evaluación (test)]{style=\"color:#FF9933; font-weight:bold\"}. En general se habla de particiones para el entrenamiento del modelo pero en este caso se trabajará con 2 particiones para el ajuste del procesamiento y modelado en conjunto. \n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "particiones",
      "metadata": {
        "results": "asis"
      },
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| code-fold: true\n",
        "#| code-summary: Generación de particiones\n",
        "#| label: particiones\n",
        "y = df[target]\n",
        "X = df.drop([target], axis=1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, shuffle=True, stratify=y, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b4446f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# | echo: false\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "y_train = y_train.reset_index(drop=True)\n",
        "y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "for particion, (color, X, y) in {\n",
        "    \"entrenamiento\": (\"#4D9900\", X_train, y_train),\n",
        "    \"evaluación\": (\"#FF9933\", X_test, y_test),\n",
        "}.items():\n",
        "    particion_string = (\n",
        "        f'<span style=\"color:{color}; font-weight:bold;\">{particion}</span>'\n",
        "    )\n",
        "    display(\n",
        "        Markdown(\n",
        "            f\"N observaciones para {particion_string}: {X.shape[0]} \"\n",
        "            f\"({y.sum() / len(y):.2%} de transacciones fraudulentas)\"\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "498e9533",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "# Preprocesamiento\n",
        "\n",
        "## Tipos de transformaciones\n",
        "\n",
        "> Ciertos tipos de transformaciones requieren `“aprender”` algunos aspectos de los [datos de entrenamiento]{style=\"color:#4D9900\"} mientras que otras no.\n",
        "\n",
        "Ejemplos de transformaciones que dependen de los datos de entrenamiento :\n",
        "\n",
        "-   Imputación de valores faltantes con la mediana → La mediana depende de los datos\n",
        "\n",
        "-   Escalado → Se debe calcular la media y desvío estándar de los datos\n",
        "\n",
        "Ejemplos de transformaciones que no dependen de los datos de entrenamiento:\n",
        "\n",
        "-   Construcción de una nueva variable mediante un cálculo simple → x\\^2\n",
        "\n",
        "-   Combinaciones de variables en una nueva variable → x/y\n",
        "\n",
        "## Transformaciones en python, mediante scikit-learn\n",
        "\n",
        "::: {style=\"font-size: 70%;\"}\n",
        "Para implementar este tipo de aprendizajes de ciertos aspectos de los datos al generar transformaciones custom, en scikit-learn se utiliza una clase específica [TransformerMixin](https://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html).\n",
        "\n",
        "![CustomTransformers en scikit-learn](images/diagrama_transformers.svg){#fig-transformers style=\"width:85%; height:auto;\"}\n",
        ":::\n",
        "\n",
        "## Transformaciones iniciales\n",
        "\n",
        "::: {style=\"font-size: 50%;\"}\n",
        "-   Cálculo de la edad\n",
        "\n",
        "-   Cálculo de la distancia entre el comercio y el usuario\n",
        "\n",
        "-   Generación de variables vinculadas a la fecha y hora de la transacción\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48b80212",
      "metadata": {},
      "outputs": [],
      "source": [
        "# | echo: false\n",
        "def categorizar_hora(hora: int) -> str:\n",
        "    \"\"\"\n",
        "    Discretizar la hora en momentos del día.\n",
        "    \"\"\"\n",
        "    if 0 <= hora < 6:\n",
        "        return \"madrugada\"\n",
        "    elif 6 <= hora < 12:\n",
        "        return \"mañana\"\n",
        "    elif 12 <= hora < 19:\n",
        "        return \"tarde\"\n",
        "    else:\n",
        "        return \"noche\"\n",
        "\n",
        "\n",
        "def calcular_distancia_haversine(lat1, lon1, lat2, lon2, radio_tierra=6371):\n",
        "    \"\"\"\n",
        "    Calcular la distancia Haversine entre dos pares de coordenadas (lat, lon)\n",
        "    \"\"\"\n",
        "    R = 6371  # Radio de la Tierra en KMs\n",
        "\n",
        "    lat1 = np.radians(lat1)\n",
        "    lon1 = np.radians(lon1)\n",
        "    lat2 = np.radians(lat2)\n",
        "    lon2 = np.radians(lon2)\n",
        "\n",
        "    dlat = lat2 - lat1\n",
        "    dlon = lon2 - lon1\n",
        "\n",
        "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
        "    c = 2 * np.arcsin(np.sqrt(a))\n",
        "\n",
        "    return R * c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "transformaciones-iniciales",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| code-fold: true\n",
        "#| code-summary: Transformaciones iniciales\n",
        "#| label: transformaciones-iniciales\n",
        "class TransformacionesIniciales(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, timestamp_features=True, distance_features=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            timestamp_features (bool): Generar variables basadas en la fecha/hora de la trx\n",
        "            distance_features (bool): Generar variables basadas en la distancia al comercio\n",
        "        \"\"\"\n",
        "        self.timestamp_features = distance_features\n",
        "        self.distance_features = distance_features\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        X_ = X.copy() # Copia para no afectar al df original\n",
        "\n",
        "        # Casteo de variables\n",
        "        X_ = X_.assign(\n",
        "            dob = lambda x: pd.to_datetime(x['dob'], errors='coerce'),\n",
        "            trans_date_trans_time = lambda x: pd.to_datetime(\n",
        "                x[\"trans_date_trans_time\"], errors=\"coerce\"\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        # Cálculo de edad\n",
        "        X_ = X_.assign(\n",
        "            age = lambda x: round((x['trans_date_trans_time']-x['dob']).dt.days / 365.25,2)\n",
        "        )\n",
        "\n",
        "        # Features basadas en fecha y hora de la trx:\n",
        "        if self.timestamp_features:\n",
        "            X_ = X_.assign(\n",
        "                trans_date__year = lambda x: x[\"trans_date_trans_time\"].dt.year,\n",
        "                trans_date__month = lambda x: x[\"trans_date_trans_time\"].dt.month,\n",
        "                trans_date__day = lambda x: x[\"trans_date_trans_time\"].dt.day,\n",
        "                trans_date__dow = lambda x: x[\"trans_date_trans_time\"].dt.dayofweek,\n",
        "                trans_date__hour = lambda x: x[\"trans_date_trans_time\"].dt.hour,\n",
        "                trans_date__partofday = lambda x: x['trans_date__hour'].apply(categorizar_hora)\n",
        "            )\n",
        "\n",
        "        if self.distance_features:\n",
        "            # Distancia (en kilometros)\n",
        "            X_[\"distance_to_merch\"] = calcular_distancia_haversine(\n",
        "                X_[\"lat\"], X_[\"long\"], X_[\"merch_lat\"], X_[\"merch_long\"]\n",
        "            )\n",
        "\n",
        "        X_ = X_.drop(['trans_date_trans_time','dob'], axis=1) \n",
        "        return X_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "transformaciones-iniciales-fittransform",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| code-fold: true\n",
        "#| code-summary: .fit_transform()\n",
        "#| label: transformaciones-iniciales-fittransform\n",
        "transformaciones = TransformacionesIniciales()\n",
        "transformaciones.fit(X_train)\n",
        "X_test_transformed = transformaciones.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6be0766d",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: false\n",
        "(GT(X_test_transformed.sample(4, random_state=42).round(2))\n",
        "    .fmt_currency(columns=\"amt\")\n",
        "    .tab_options(\n",
        "        column_labels_background_color=color_verde,\n",
        "        table_font_names=\"Times New Roman\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9309eab8",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## Feature engineering\n",
        "\n",
        "::: {style=\"font-size: 50%;\"}\n",
        "\n",
        "Scikit-learn cuenta con múltiples transformers ya definidos (SimpleImput, MinMaxScaler, etc). Además, se definen 4 transformers custom adicionales (`MeanEncoder, RareCategoryGrouper, ColumnCapper, IsolationForestTransformer`):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "preproc-meanencoder",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| code-fold: true\n",
        "#| code-summary: MeanEncoder\n",
        "#| label: preproc-meanencoder\n",
        "class MeanEncoder(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Encoding de variables categóricas mediante el promedio de la target en esa categoría.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, variables=None):\n",
        "        self.variables = variables\n",
        "        self.encoding_dict_ = {}\n",
        "        self.global_mean_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X_ = X.copy()\n",
        "        y_ = pd.Series(y)\n",
        "\n",
        "        if self.variables is None:\n",
        "            self.variables = X_.select_dtypes(\n",
        "                include=[\"object\", \"category\"]\n",
        "            ).columns.tolist()\n",
        "\n",
        "        self.global_mean_ = y_.mean()\n",
        "\n",
        "        for var in self.variables:\n",
        "            self.encoding_dict_[var] = y_.groupby(X_[var]).mean().to_dict()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_ = X.copy()\n",
        "        for var in self.variables:\n",
        "            X_[var] = X_[var].map(self.encoding_dict_[var])\n",
        "            # Para nuevas categorías asigna el valor promedio\n",
        "            X_[var] = X_[var].fillna(self.global_mean_)\n",
        "        return X_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "preproc-rarecategory",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| code-fold: true\n",
        "#| code-summary: RareCategoryGrouper\n",
        "#| label: preproc-rarecategory\n",
        "class RareCategoryGrouper(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Agrupar categorías poco frecuentes en \"infrequent\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, variables=None, min_freq=0.05):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            variables (list): List of categorical variables to group.\n",
        "            min_freq (float or int):\n",
        "                Si float (0 < min_freq < 1), mínima proporción en el dataset.\n",
        "                Si int (>=1), mínima cantidad de observaciones.\n",
        "        \"\"\"\n",
        "        self.variables = variables\n",
        "        self.min_freq = min_freq\n",
        "        self.frequent_categories_ = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X_ = X.copy()\n",
        "\n",
        "        if self.variables is None:\n",
        "            self.variables = X_.select_dtypes(include=\"object\").columns.tolist()\n",
        "\n",
        "        for var in self.variables:\n",
        "            freqs = X_[var].value_counts(normalize=isinstance(self.min_freq, float))\n",
        "            self.frequent_categories_[var] = freqs[\n",
        "                freqs >= self.min_freq\n",
        "            ].index.tolist()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_ = X.copy()\n",
        "        for var in self.variables:\n",
        "            X_[var] = X_[var].where(\n",
        "                X_[var].isin(self.frequent_categories_[var]), \"infrequent\"\n",
        "            )\n",
        "        return X_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "preproc-outliers",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| code-fold: true\n",
        "#| code-summary: ColumnCapper\n",
        "#| label: preproc-outliers\n",
        "class ColumnCapper(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Cappear las variables mediante el método de IQR\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, numeric_features=None, factor=1.5):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            numeric_features (list): lista de variables sobre las cuales remover valores atípicos\n",
        "            factor (float): multiplicador del IQR\n",
        "        \"\"\"\n",
        "        self.numeric_features = numeric_features\n",
        "        self.factor = factor\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X_ = X.copy()\n",
        "        if self.numeric_features is None:\n",
        "            self.numeric_features = X_.select_dtypes(include=\"number\").columns.tolist()\n",
        "        # IQR\n",
        "        Q1 = X_[self.numeric_features].quantile(0.25)\n",
        "        Q3 = X_[self.numeric_features].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        self.lower_bound = Q1 - self.factor * IQR\n",
        "        self.upper_bound = Q3 + self.factor * IQR\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        X_ = X.copy()\n",
        "        for col in self.numeric_features:\n",
        "            X_[col] = X_[col].clip(\n",
        "                lower=self.lower_bound[col], \n",
        "                upper=self.upper_bound[col]\n",
        "            )\n",
        "        return X_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "preproc-anomaly",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| code-fold: true\n",
        "#| code-summary: IsolationForestTransformer\n",
        "#| label: preproc-anomaly\n",
        "class IsolationForestTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Ajustar un modelo IssolationForest para detección de anomalías. \n",
        "    Retorna un anomaly_score. \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        self.iforest_kwargs = kwargs\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.iforest_ = IsolationForest(**self.iforest_kwargs)\n",
        "        self.iforest_.fit(X)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        scores = self.iforest_.decision_function(X)\n",
        "        return pd.DataFrame({\"anomaly_score\": scores}, index=X.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "367b58f1",
      "metadata": {},
      "source": [
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04550fde",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "::: {style=\"font-size: 50%;\"}\n",
        "\n",
        "Se crea un Pipeline de preprocesamiento general:\n",
        "\n",
        "1. Transformaciones iniciales\n",
        "\n",
        "2. Transformer que según el tipo de variable (numérica o categórica) aplica ciertas transformaciones:\n",
        "\n",
        "    - Pipeline para procesamiento de variables categóricas\n",
        "    - Pipeline para procesamiento de variables numéricas\n",
        "\n",
        "3. Identificación de observaciones anómalas y cappeo de variables según el IQR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "preprocessor",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| code-fold: true\n",
        "#| code-summary: Preprocesamiento\n",
        "#| label: preprocessor\n",
        "preproc_categoricas = Pipeline(steps=[\n",
        "    ('rare_labels', RareCategoryGrouper(min_freq=0.01)),\n",
        "    ('imputar_nulos', SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n",
        "    ('mean_encoder', MeanEncoder())\n",
        "])\n",
        "\n",
        "preproc_numericas = Pipeline(steps=[\n",
        "    ('imputar_nulos', SimpleImputer(strategy='median')),\n",
        "    ('scale', MinMaxScaler())\n",
        "])\n",
        "\n",
        "feature_eng = ColumnTransformer([\n",
        "    ('cat', preproc_categoricas, make_column_selector(dtype_exclude=['float','int'])),\n",
        "    ('num', preproc_numericas, make_column_selector(dtype_include=['float','int']))\n",
        "], verbose_feature_names_out=False, remainder='drop', verbose=True)\n",
        "\n",
        "features_preprocessing = Pipeline([\n",
        "    ('data_cleaning', TransformacionesIniciales()),\n",
        "    ('feature_eng', feature_eng),\n",
        "    ('anomalies', FeatureUnion([\n",
        "            ('outliers', ColumnCapper()),\n",
        "            ('anomaly', IsolationForestTransformer())\n",
        "        ])\n",
        "    )\n",
        "], verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4dce528",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: false\n",
        "preproc_html = features_preprocessing._repr_html_()\n",
        "with open(\"artifacts/preproc_display.html\", \"w\") as f:\n",
        "    f.write(preproc_html)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa820aad",
      "metadata": {},
      "source": [
        "<iframe src=\"artifacts/preproc_display.html\" width=\"100%\" height=\"600px\" style=\"border: none;\">\n",
        "</iframe>\n",
        "\n",
        ":::\n",
        "\n",
        "## Datos transformados\n",
        "\n",
        "::: {style=\"font-size: 75%;\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "preproc-transform",
      "metadata": {
        "message": false
      },
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| code-fold: true\n",
        "#| code-summary: .fit_transform()\n",
        "#| label: preproc-transform\n",
        "#| warning: false\n",
        "features_preprocessing.fit(X_train, y_train)\n",
        "X_test_transformed = features_preprocessing.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c683007c",
      "metadata": {},
      "source": [
        "<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tbl-postprocessing-data",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: false\n",
        "#| tbl-cap: Datos post procesamiento (muestra de 4 observaciones de la partición de evaluación)\n",
        "#| label: tbl-postprocessing-data\n",
        "(GT(X_test_transformed.sample(4, random_state=42).round(3))\n",
        "    .tab_options(\n",
        "        column_labels_background_color=color_verde,\n",
        "        table_font_names=\"Times New Roman\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34195412",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "# Modelo\n",
        "\n",
        "## Pipeline de modelado\n",
        "\n",
        "Se crea un nuevo Pipeline que incluye un primer paso de preprocesamiento y un segundo paso de modelado (un clasificador). \n",
        "\n",
        "::: {style=\"font-size: 70%;\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pipeline",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| code-fold: true\n",
        "#| code-summary: Pipeline\n",
        "#| label: pipeline\n",
        "#| eval: true\n",
        "clf = CatBoostClassifier(\n",
        "    iterations=500,\n",
        "    loss_function=\"Logloss\",\n",
        "    class_weights=[1, 20],\n",
        "    random_seed=42,\n",
        "    verbose=100,\n",
        ")\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"preproc\", features_preprocessing), \n",
        "    (\"model\", clf)\n",
        "], verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f09a6af5",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: false\n",
        "#| eval: false\n",
        "pipe_html = pipe._repr_html_()\n",
        "with open(\"artifacts/pipe_display.html\", \"w\") as f:\n",
        "    f.write(pipe_html)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4956797",
      "metadata": {},
      "source": [
        "<iframe src=\"artifacts/pipe_display.html\" width=\"100%\" height=\"600px\" style=\"border: none;\">\n",
        "\n",
        "</iframe>\n",
        ":::\n",
        "\n",
        "## Entrenamiento del pipeline completo\n",
        "\n",
        "::: {style=\"font-size: 75%;\"}\n",
        "Durante el entrenamiento del pipeline (preprocesamiento + modelo), se visualizan los tiempos que tarda cada uno de los pasos:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a320431",
      "metadata": {
        "message": false
      },
      "outputs": [],
      "source": [
        "#| eval: true\n",
        "#| echo: true\n",
        "#| warning: false\n",
        "pipe.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "360eabcb",
      "metadata": {
        "results": "asis"
      },
      "outputs": [],
      "source": [
        "#| eval: false\n",
        "print(f\"\"\"\n",
        "[Pipeline] ..... (step 1 of 3) Processing data_cleaning, total=   1.9s\n",
        "[ColumnTransformer] ........... (1 of 2) Processing cat, total=   2.4s\n",
        "[ColumnTransformer] ........... (2 of 2) Processing num, total=   2.7s\n",
        "[Pipeline] ....... (step 2 of 3) Processing feature_eng, total=   5.3s\n",
        "[Pipeline] ......... (step 3 of 3) Processing anomalies, total=  10.8s\n",
        "0:\t    learn: 0.4501382\ttotal: 98.1ms\tremaining: 48.9s\n",
        "100:\tlearn: 0.0492939\ttotal: 13.2s\tremaining: 52.3s\n",
        "200:\tlearn: 0.0390136\ttotal: 26.7s\tremaining: 39.8s\n",
        "300:\tlearn: 0.0324107\ttotal: 39.1s\tremaining: 25.9s\n",
        "400:\tlearn: 0.0280930\ttotal: 52.5s\tremaining: 13s\n",
        "499:\tlearn: 0.0247809\ttotal: 1m 5s\tremaining: 0us\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "111b098a",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: false\n",
        "#| eval: false\n",
        "df_preds_test = pd.DataFrame({\n",
        "    'y_true': y_test.reset_index(drop=True),\n",
        "    'y_pred_class': pipe.predict(X_test),\n",
        "    'y_pred_prob': round(pd.Series(pipe.predict_proba(X_test)[:,1]),4)\n",
        "})\n",
        "index_fraud_prob = df_preds_test.sort_values('y_pred_prob', ascending=False).head(1).index\n",
        "\n",
        "X_test.iloc[index_fraud_prob].to_dict(orient='index')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce468760",
      "metadata": {},
      "source": [
        "Almacenar el modelo para luego utilizarlo (despliegue):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "save-model",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "#| code-fold: true\n",
        "#| code-summary: 'pickle.dump(): Guardar el modelo'\n",
        "#| label: save-model\n",
        "with open('artifacts/pipe_model_fraud.pkl', 'wb') as file:\n",
        "    pickle.dump(pipe, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b3a2ca2",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "# Predicciones\n",
        "\n",
        "## Predicciones sobre datos nuevos (despliegue de modelos)\n",
        "\n",
        "::: {style=\"font-size: 50%;\"}\n",
        "Cargar el archivo .pkl para utilizarlo:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-model",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| eval: true\n",
        "#| code-fold: true\n",
        "#| code-summary: 'pickle.load(): Cargar el modelo'\n",
        "#| label: load-model\n",
        "with open('artifacts/pipe_model_fraud.pkl', 'rb') as file:\n",
        "    pipe = pickle.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a01c8c52",
      "metadata": {},
      "source": [
        "🆕 Datos de una nueva transacción ([datos en producción]{style=\"color:#3399FF\"}):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "new-trx",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| eval: true\n",
        "#| code-fold: true\n",
        "#| code-summary: Nueva transacción\n",
        "#| label: new-trx\n",
        "nueva_trx = pd.DataFrame({\n",
        "    \"trans_date_trans_time\": \"2019-10-09 20:38:49\",\n",
        "    \"merchant\": np.nan,\n",
        "    \"category\": \"gas_transport\",\n",
        "    \"amt\": 9.66,\n",
        "    \"city_pop\": 10000,\n",
        "    \"job\": np.nan,\n",
        "    \"dob\": \"1995-08-16\",\n",
        "    \"zip\": np.nan,\n",
        "    \"lat\": 45.8433,\n",
        "    \"long\": -113.1948,\n",
        "    \"merch_lat\": 45.837213,\n",
        "    \"merch_long\": -113.191425,\n",
        "}, index=[\"nueva_trx\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7e7ab63",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: false\n",
        "(GT(nueva_trx.round(2))\n",
        "    .fmt_currency(columns=\"amt\")\n",
        "    .tab_options(\n",
        "        column_labels_background_color=color_verde,\n",
        "        table_font_names=\"Times New Roman\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52cb8188",
      "metadata": {},
      "source": [
        "Utilizar el modelo para estimar la `probabilidad` de que la nueva transacción sea fraudulenta:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "new-prediction",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| eval: true\n",
        "#| code-fold: true\n",
        "#| code-summary: Predicción de probabilidad\n",
        "#| label: new-prediction\n",
        "\n",
        "y_pred = pipe.predict_proba(nueva_trx)[:,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a362696f",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: false\n",
        "Markdown(f\"La probabilidad de que la transacción sea fraudulenta es: {y_pred[0]:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d39e430",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "# Comentarios finales\n",
        "\n",
        "## Comentarios finales\n",
        "\n",
        "-   El uso de pipelines en python permite organizar el flujo de procesamiento de datos de manera clara, reproducible y escalable.\n",
        "\n",
        "-   Esto permite reducir el riesgo durante el despliegue de modelos al eliminar código duplicado o transformaciones inconsistentes entre entrenamiento y predicción.\n",
        "\n",
        "-   Todas las transformaciones mostradas son ejemplos ilustrativos, otras transformaciones podrían generar mejores resultados.\n",
        "\n",
        "## Referencias / Recursos\n",
        "\n",
        "::: {#refs}\n",
        ":::\n",
        "\n",
        "## Contacto\n",
        "\n",
        "{{< fa brands linkedin size=1x >}} [karinabartolome](https://www.linkedin.com/in/karinabartolome/)\n",
        "\n",
        "{{< fa brands twitter size=1x >}} [karbartolome](https://twitter.com/karbartolome)\n",
        "\n",
        "{{< fa brands github size=1x >}} [karbartolome](http://github.com/karbartolome)\n",
        "\n",
        "{{< fa link >}} [Blog](https://karbartolome-blog.netlify.app/)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base-ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
